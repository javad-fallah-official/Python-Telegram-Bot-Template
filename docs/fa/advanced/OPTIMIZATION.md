# âš¡ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯

> **Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ø±Ø¨Ø§Øª ØªÙ„Ú¯Ø±Ø§Ù… Ø¨Ø±Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ø§Ù„Ø§ Ùˆ Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ**

## ğŸ¯ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ

Ø§ÛŒÙ† Ø±Ø§Ù‡Ù†Ù…Ø§ Ø´Ø§Ù…Ù„ ØªÙ…Ø§Ù… ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø¨Ø§Øª ØªÙ„Ú¯Ø±Ø§Ù… Ø§Ø² Ø¬Ù…Ù„Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø¯ØŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ Ø­Ø§ÙØ¸Ù‡ØŒ Ø´Ø¨Ú©Ù‡ Ùˆ Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø§Ø³Øª.

## ğŸ“Š Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯

### 1. Profiling Ùˆ Benchmarking

#### CPU Profiling
```python
# tools/profiler.py
import cProfile
import pstats
import io
from functools import wraps
import time
import asyncio

def profile_function(func):
    """Decorator Ø¨Ø±Ø§ÛŒ profiling ØªÙˆØ§Ø¨Ø¹"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        pr = cProfile.Profile()
        pr.enable()
        
        result = func(*args, **kwargs)
        
        pr.disable()
        s = io.StringIO()
        ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
        ps.print_stats()
        
        print(f"Profile for {func.__name__}:")
        print(s.getvalue())
        
        return result
    return wrapper

def profile_async_function(func):
    """Decorator Ø¨Ø±Ø§ÛŒ profiling ØªÙˆØ§Ø¨Ø¹ async"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        pr = cProfile.Profile()
        pr.enable()
        
        result = await func(*args, **kwargs)
        
        pr.disable()
        s = io.StringIO()
        ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
        ps.print_stats()
        
        print(f"Async Profile for {func.__name__}:")
        print(s.getvalue())
        
        return result
    return wrapper

# Ø§Ø³ØªÙØ§Ø¯Ù‡
@profile_async_function
async def process_message(message):
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ§Ù…
    await asyncio.sleep(0.1)
    return "processed"
```

#### Memory Profiling
```python
# tools/memory_profiler.py
import tracemalloc
import psutil
import os
from functools import wraps

class MemoryProfiler:
    """Ú©Ù„Ø§Ø³ profiling Ø­Ø§ÙØ¸Ù‡"""
    
    def __init__(self):
        self.snapshots = []
    
    def start_tracing(self):
        """Ø´Ø±ÙˆØ¹ Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ø­Ø§ÙØ¸Ù‡"""
        tracemalloc.start()
    
    def take_snapshot(self, name: str):
        """Ú¯Ø±ÙØªÙ† snapshot Ø§Ø² Ø­Ø§ÙØ¸Ù‡"""
        snapshot = tracemalloc.take_snapshot()
        self.snapshots.append((name, snapshot))
    
    def compare_snapshots(self, name1: str, name2: str):
        """Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ùˆ snapshot"""
        snap1 = next((s for n, s in self.snapshots if n == name1), None)
        snap2 = next((s for n, s in self.snapshots if n == name2), None)
        
        if snap1 and snap2:
            top_stats = snap2.compare_to(snap1, 'lineno')
            
            print(f"Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø­Ø§ÙØ¸Ù‡: {name1} -> {name2}")
            for stat in top_stats[:10]:
                print(stat)
    
    def get_current_memory_usage(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ù…ØµØ±Ù ÙØ¹Ù„ÛŒ Ø­Ø§ÙØ¸Ù‡"""
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        
        return {
            'rss': memory_info.rss / 1024 / 1024,  # MB
            'vms': memory_info.vms / 1024 / 1024,  # MB
            'percent': process.memory_percent()
        }

def memory_usage(func):
    """Decorator Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        profiler = MemoryProfiler()
        
        # Ø­Ø§ÙØ¸Ù‡ Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø¬Ø±Ø§
        before = profiler.get_current_memory_usage()
        
        result = await func(*args, **kwargs)
        
        # Ø­Ø§ÙØ¸Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ø¬Ø±Ø§
        after = profiler.get_current_memory_usage()
        
        print(f"Memory usage for {func.__name__}:")
        print(f"  Before: {before['rss']:.2f} MB")
        print(f"  After: {after['rss']:.2f} MB")
        print(f"  Diff: {after['rss'] - before['rss']:.2f} MB")
        
        return result
    return wrapper
```

#### Performance Monitoring
```python
# monitoring/performance.py
import time
import asyncio
from collections import defaultdict, deque
from datetime import datetime, timedelta
import statistics

class PerformanceMonitor:
    """Ù…Ø§Ù†ÛŒØªÙˆØ± Ø¹Ù…Ù„Ú©Ø±Ø¯"""
    
    def __init__(self, window_size: int = 1000):
        self.window_size = window_size
        self.metrics = defaultdict(lambda: deque(maxlen=window_size))
        self.counters = defaultdict(int)
    
    def record_timing(self, operation: str, duration: float):
        """Ø«Ø¨Øª Ø²Ù…Ø§Ù† Ø¹Ù…Ù„ÛŒØ§Øª"""
        self.metrics[f"{operation}_timing"].append(duration)
    
    def increment_counter(self, counter: str):
        """Ø§ÙØ²Ø§ÛŒØ´ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡"""
        self.counters[counter] += 1
    
    def get_stats(self, operation: str) -> dict:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø¹Ù…Ù„ÛŒØ§Øª"""
        timings = list(self.metrics[f"{operation}_timing"])
        
        if not timings:
            return {}
        
        return {
            'count': len(timings),
            'avg': statistics.mean(timings),
            'median': statistics.median(timings),
            'min': min(timings),
            'max': max(timings),
            'p95': statistics.quantiles(timings, n=20)[18] if len(timings) > 20 else max(timings),
            'p99': statistics.quantiles(timings, n=100)[98] if len(timings) > 100 else max(timings)
        }
    
    def get_throughput(self, counter: str, window_minutes: int = 5) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ throughput"""
        # Ø§ÛŒÙ† ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ Ø§Ø³Øª
        # Ø¯Ø± Ø¹Ù…Ù„ Ø¨Ø§ÛŒØ¯ Ø§Ø² time-based windows Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯
        return self.counters[counter] / window_minutes

# Global monitor instance
perf_monitor = PerformanceMonitor()

def monitor_performance(operation: str):
    """Decorator Ø¨Ø±Ø§ÛŒ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            
            try:
                result = await func(*args, **kwargs)
                perf_monitor.increment_counter(f"{operation}_success")
                return result
            except Exception as e:
                perf_monitor.increment_counter(f"{operation}_error")
                raise
            finally:
                duration = time.time() - start_time
                perf_monitor.record_timing(operation, duration)
        
        return wrapper
    return decorator
```

## ğŸš€ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø¯

### 1. Async/Await Optimization

#### Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Concurrent Operations
```python
# optimizations/async_utils.py
import asyncio
from typing import List, Callable, Any
import aiohttp
from functools import wraps

class AsyncBatch:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ batch async"""
    
    def __init__(self, batch_size: int = 10, delay: float = 0.1):
        self.batch_size = batch_size
        self.delay = delay
    
    async def process_batch(self, items: List[Any], processor: Callable):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ batch Ø§Ø² Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§"""
        results = []
        
        for i in range(0, len(items), self.batch_size):
            batch = items[i:i + self.batch_size]
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ batch
            batch_tasks = [processor(item) for item in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            results.extend(batch_results)
            
            # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† batch Ù‡Ø§
            if i + self.batch_size < len(items):
                await asyncio.sleep(self.delay)
        
        return results

async def send_messages_optimized(bot, chat_ids: List[int], text: str):
    """Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ÛŒÙ†Ù‡ Ù¾ÛŒØ§Ù… Ø¨Ù‡ Ú†Ù†Ø¯ÛŒÙ† Ú©Ø§Ø±Ø¨Ø±"""
    
    async def send_single_message(chat_id: int):
        try:
            return await bot.send_message(chat_id, text)
        except Exception as e:
            return f"Error for {chat_id}: {e}"
    
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² batch processing
    batch_processor = AsyncBatch(batch_size=20, delay=0.05)
    results = await batch_processor.process_batch(chat_ids, send_single_message)
    
    return results

# Connection pooling Ø¨Ø±Ø§ÛŒ HTTP requests
class OptimizedHTTPClient:
    """HTTP client Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡"""
    
    def __init__(self):
        self.connector = aiohttp.TCPConnector(
            limit=100,  # Ø­Ø¯Ø§Ú©Ø«Ø± connection
            limit_per_host=30,  # Ø­Ø¯Ø§Ú©Ø«Ø± connection per host
            ttl_dns_cache=300,  # DNS cache TTL
            use_dns_cache=True,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )
        
        self.timeout = aiohttp.ClientTimeout(
            total=30,
            connect=10,
            sock_read=10
        )
        
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=self.timeout
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def get(self, url: str, **kwargs):
        """GET request Ø¨Ù‡ÛŒÙ†Ù‡"""
        async with self.session.get(url, **kwargs) as response:
            return await response.json()
    
    async def post(self, url: str, **kwargs):
        """POST request Ø¨Ù‡ÛŒÙ†Ù‡"""
        async with self.session.post(url, **kwargs) as response:
            return await response.json()
```

#### Task Management
```python
# optimizations/task_manager.py
import asyncio
import weakref
from typing import Set, Optional
import logging

class TaskManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª task Ù‡Ø§ÛŒ async"""
    
    def __init__(self):
        self._tasks: Set[asyncio.Task] = set()
        self._background_tasks: Set[asyncio.Task] = set()
    
    def create_task(self, coro, *, name: Optional[str] = None, background: bool = False) -> asyncio.Task:
        """Ø§ÛŒØ¬Ø§Ø¯ task Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®ÙˆØ¯Ú©Ø§Ø±"""
        task = asyncio.create_task(coro, name=name)
        
        if background:
            self._background_tasks.add(task)
            task.add_done_callback(self._background_tasks.discard)
        else:
            self._tasks.add(task)
            task.add_done_callback(self._tasks.discard)
        
        # Log Ú©Ø±Ø¯Ù† exception Ù‡Ø§
        task.add_done_callback(self._log_task_exception)
        
        return task
    
    def _log_task_exception(self, task: asyncio.Task):
        """Log Ú©Ø±Ø¯Ù† exception Ù‡Ø§ÛŒ task"""
        if task.done() and not task.cancelled():
            exception = task.exception()
            if exception:
                logging.error(f"Task {task.get_name()} failed: {exception}")
    
    async def shutdown(self, timeout: float = 10.0):
        """Ø®Ø§Ù…ÙˆØ´ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… task Ù‡Ø§"""
        # Ù„ØºÙˆ task Ù‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ
        for task in self._tasks:
            task.cancel()
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ ØªÚ©Ù…ÛŒÙ„
        if self._tasks:
            await asyncio.wait(self._tasks, timeout=timeout)
        
        # Ù„ØºÙˆ task Ù‡Ø§ÛŒ background
        for task in self._background_tasks:
            task.cancel()
        
        if self._background_tasks:
            await asyncio.wait(self._background_tasks, timeout=timeout)
    
    def get_running_tasks_count(self) -> dict:
        """Ø¯Ø±ÛŒØ§ÙØª ØªØ¹Ø¯Ø§Ø¯ task Ù‡Ø§ÛŒ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§"""
        return {
            'normal_tasks': len(self._tasks),
            'background_tasks': len(self._background_tasks),
            'total': len(self._tasks) + len(self._background_tasks)
        }

# Global task manager
task_manager = TaskManager()
```

### 2. Caching Strategies

#### Memory Caching
```python
# optimizations/cache.py
import asyncio
import time
from typing import Any, Optional, Callable, Dict
from functools import wraps
import weakref
import pickle
import hashlib

class MemoryCache:
    """Cache Ø­Ø§ÙØ¸Ù‡ Ø¨Ø§ TTL"""
    
    def __init__(self, default_ttl: int = 300, max_size: int = 1000):
        self.default_ttl = default_ttl
        self.max_size = max_size
        self._cache: Dict[str, tuple] = {}  # key: (value, expire_time)
        self._access_times: Dict[str, float] = {}
    
    def _is_expired(self, key: str) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†Ù‚Ø¶Ø§ÛŒ Ú©Ù„ÛŒØ¯"""
        if key not in self._cache:
            return True
        
        _, expire_time = self._cache[key]
        return time.time() > expire_time
    
    def _evict_expired(self):
        """Ø­Ø°Ù Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ù‚Ø¶ÛŒ Ø´Ø¯Ù‡"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, expire_time) in self._cache.items()
            if current_time > expire_time
        ]
        
        for key in expired_keys:
            self._cache.pop(key, None)
            self._access_times.pop(key, None)
    
    def _evict_lru(self):
        """Ø­Ø°Ù Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ú©Ù…ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ (LRU)"""
        if len(self._cache) <= self.max_size:
            return
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù† Ø¯Ø³ØªØ±Ø³ÛŒ
        sorted_keys = sorted(
            self._access_times.items(),
            key=lambda x: x[1]
        )
        
        # Ø­Ø°Ù Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§
        keys_to_remove = sorted_keys[:len(self._cache) - self.max_size]
        for key, _ in keys_to_remove:
            self._cache.pop(key, None)
            self._access_times.pop(key, None)
    
    def get(self, key: str) -> Optional[Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø² cache"""
        if self._is_expired(key):
            self._cache.pop(key, None)
            self._access_times.pop(key, None)
            return None
        
        self._access_times[key] = time.time()
        value, _ = self._cache[key]
        return value
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None):
        """ØªÙ†Ø¸ÛŒÙ… Ø¯Ø± cache"""
        if ttl is None:
            ttl = self.default_ttl
        
        expire_time = time.time() + ttl
        self._cache[key] = (value, expire_time)
        self._access_times[key] = time.time()
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
        self._evict_expired()
        self._evict_lru()
    
    def delete(self, key: str):
        """Ø­Ø°Ù Ø§Ø² cache"""
        self._cache.pop(key, None)
        self._access_times.pop(key, None)
    
    def clear(self):
        """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú©Ø§Ù…Ù„ cache"""
        self._cache.clear()
        self._access_times.clear()
    
    def stats(self) -> dict:
        """Ø¢Ù…Ø§Ø± cache"""
        return {
            'size': len(self._cache),
            'max_size': self.max_size,
            'hit_ratio': getattr(self, '_hits', 0) / max(getattr(self, '_requests', 1), 1)
        }

# Global cache instance
memory_cache = MemoryCache()

def cached(ttl: int = 300, key_func: Optional[Callable] = None):
    """Decorator Ø¨Ø±Ø§ÛŒ caching"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ÛŒØ¯ cache
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² hash args Ùˆ kwargs
                key_data = pickle.dumps((args, sorted(kwargs.items())))
                cache_key = f"{func.__name__}:{hashlib.md5(key_data).hexdigest()}"
            
            # Ø¨Ø±Ø±Ø³ÛŒ cache
            cached_result = memory_cache.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± cache
            result = await func(*args, **kwargs)
            memory_cache.set(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator
```

#### Redis Caching
```python
# optimizations/redis_cache.py
import redis.asyncio as redis
import json
import pickle
from typing import Any, Optional, Union
import logging

class RedisCache:
    """Redis cache Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379/0"):
        self.redis_url = redis_url
        self.redis_client = None
    
    async def connect(self):
        """Ø§ØªØµØ§Ù„ Ø¨Ù‡ Redis"""
        self.redis_client = redis.from_url(
            self.redis_url,
            encoding="utf-8",
            decode_responses=False,  # Ø¨Ø±Ø§ÛŒ pickle
            max_connections=20,
            retry_on_timeout=True
        )
    
    async def disconnect(self):
        """Ù‚Ø·Ø¹ Ø§ØªØµØ§Ù„"""
        if self.redis_client:
            await self.redis_client.close()
    
    async def get(self, key: str) -> Optional[Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø² Redis"""
        try:
            data = await self.redis_client.get(key)
            if data:
                return pickle.loads(data)
            return None
        except Exception as e:
            logging.error(f"Redis get error: {e}")
            return None
    
    async def set(self, key: str, value: Any, ttl: int = 300):
        """ØªÙ†Ø¸ÛŒÙ… Ø¯Ø± Redis"""
        try:
            data = pickle.dumps(value)
            await self.redis_client.setex(key, ttl, data)
        except Exception as e:
            logging.error(f"Redis set error: {e}")
    
    async def delete(self, key: str):
        """Ø­Ø°Ù Ø§Ø² Redis"""
        try:
            await self.redis_client.delete(key)
        except Exception as e:
            logging.error(f"Redis delete error: {e}")
    
    async def exists(self, key: str) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ú©Ù„ÛŒØ¯"""
        try:
            return await self.redis_client.exists(key) > 0
        except Exception as e:
            logging.error(f"Redis exists error: {e}")
            return False
    
    async def increment(self, key: str, amount: int = 1) -> int:
        """Ø§ÙØ²Ø§ÛŒØ´ Ù…Ù‚Ø¯Ø§Ø±"""
        try:
            return await self.redis_client.incrby(key, amount)
        except Exception as e:
            logging.error(f"Redis increment error: {e}")
            return 0
    
    async def get_many(self, keys: list) -> dict:
        """Ø¯Ø±ÛŒØ§ÙØª Ú†Ù†Ø¯ÛŒÙ† Ú©Ù„ÛŒØ¯"""
        try:
            values = await self.redis_client.mget(keys)
            result = {}
            for key, value in zip(keys, values):
                if value:
                    result[key] = pickle.loads(value)
            return result
        except Exception as e:
            logging.error(f"Redis get_many error: {e}")
            return {}
    
    async def set_many(self, mapping: dict, ttl: int = 300):
        """ØªÙ†Ø¸ÛŒÙ… Ú†Ù†Ø¯ÛŒÙ† Ú©Ù„ÛŒØ¯"""
        try:
            pipe = self.redis_client.pipeline()
            for key, value in mapping.items():
                data = pickle.dumps(value)
                pipe.setex(key, ttl, data)
            await pipe.execute()
        except Exception as e:
            logging.error(f"Redis set_many error: {e}")

# Global Redis cache
redis_cache = RedisCache()
```

## ğŸ—„ï¸ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡

### 1. Query Optimization

#### Connection Pooling
```python
# optimizations/db_pool.py
import asyncpg
import asyncio
from typing import Optional, Any, List, Dict
import logging
from contextlib import asynccontextmanager

class DatabasePool:
    """Pool Ø§ØªØµØ§Ù„ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡"""
    
    def __init__(self, database_url: str, min_size: int = 10, max_size: int = 20):
        self.database_url = database_url
        self.min_size = min_size
        self.max_size = max_size
        self.pool: Optional[asyncpg.Pool] = None
    
    async def create_pool(self):
        """Ø§ÛŒØ¬Ø§Ø¯ pool Ø§ØªØµØ§Ù„"""
        self.pool = await asyncpg.create_pool(
            self.database_url,
            min_size=self.min_size,
            max_size=self.max_size,
            command_timeout=60,
            server_settings={
                'jit': 'off',  # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ query Ù‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡
                'application_name': 'telegram_bot'
            }
        )
    
    async def close_pool(self):
        """Ø¨Ø³ØªÙ† pool"""
        if self.pool:
            await self.pool.close()
    
    @asynccontextmanager
    async def acquire_connection(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø§ØªØµØ§Ù„ Ø§Ø² pool"""
        async with self.pool.acquire() as connection:
            yield connection
    
    async def execute_query(self, query: str, *args) -> List[Dict]:
        """Ø§Ø¬Ø±Ø§ÛŒ query Ø¨Ø§ connection pooling"""
        async with self.acquire_connection() as conn:
            rows = await conn.fetch(query, *args)
            return [dict(row) for row in rows]
    
    async def execute_many(self, query: str, args_list: List[tuple]):
        """Ø§Ø¬Ø±Ø§ÛŒ batch query"""
        async with self.acquire_connection() as conn:
            await conn.executemany(query, args_list)
    
    async def execute_transaction(self, queries: List[tuple]):
        """Ø§Ø¬Ø±Ø§ÛŒ transaction"""
        async with self.acquire_connection() as conn:
            async with conn.transaction():
                for query, args in queries:
                    await conn.execute(query, *args)

# Global database pool
db_pool = DatabasePool("postgresql://user:pass@localhost/db")
```

#### Query Caching
```python
# optimizations/query_cache.py
from functools import wraps
import hashlib
import json

class QueryCache:
    """Cache Ø¨Ø±Ø§ÛŒ query Ù‡Ø§ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"""
    
    def __init__(self, cache_backend=None):
        self.cache_backend = cache_backend or memory_cache
    
    def _generate_cache_key(self, query: str, args: tuple) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ÛŒØ¯ cache Ø¨Ø±Ø§ÛŒ query"""
        key_data = json.dumps({
            'query': query,
            'args': args
        }, sort_keys=True)
        return f"query:{hashlib.md5(key_data.encode()).hexdigest()}"
    
    def cached_query(self, ttl: int = 300):
        """Decorator Ø¨Ø±Ø§ÛŒ cache Ú©Ø±Ø¯Ù† query Ù‡Ø§"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ query Ùˆ args
                if 'query' in kwargs:
                    query = kwargs['query']
                    query_args = kwargs.get('args', ())
                else:
                    query = args[0] if args else ""
                    query_args = args[1:] if len(args) > 1 else ()
                
                # ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ÛŒØ¯ cache
                cache_key = self._generate_cache_key(query, query_args)
                
                # Ø¨Ø±Ø±Ø³ÛŒ cache
                cached_result = await self.cache_backend.get(cache_key)
                if cached_result is not None:
                    return cached_result
                
                # Ø§Ø¬Ø±Ø§ÛŒ query
                result = await func(*args, **kwargs)
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± cache
                await self.cache_backend.set(cache_key, result, ttl)
                
                return result
            return wrapper
        return decorator

query_cache = QueryCache()
```

#### Optimized Database Operations
```python
# optimizations/db_operations.py
from typing import List, Dict, Any, Optional
import asyncio

class OptimizedDatabase:
    """Ø¹Ù…Ù„ÛŒØ§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"""
    
    def __init__(self, pool: DatabasePool):
        self.pool = pool
    
    @query_cache.cached_query(ttl=600)
    async def get_user_stats(self, user_id: int) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ú©Ø§Ø±Ø¨Ø± Ø¨Ø§ cache"""
        query = """
        SELECT 
            u.user_id,
            u.first_name,
            u.last_name,
            u.username,
            COUNT(m.id) as message_count,
            MAX(m.created_at) as last_message_at
        FROM users u
        LEFT JOIN messages m ON u.user_id = m.user_id
        WHERE u.user_id = $1
        GROUP BY u.user_id, u.first_name, u.last_name, u.username
        """
        
        result = await self.pool.execute_query(query, user_id)
        return result[0] if result else {}
    
    async def bulk_insert_messages(self, messages: List[Dict[str, Any]]):
        """Ø¯Ø±Ø¬ bulk Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§"""
        if not messages:
            return
        
        query = """
        INSERT INTO messages (user_id, message_id, text, message_type, created_at)
        VALUES ($1, $2, $3, $4, $5)
        ON CONFLICT (user_id, message_id) DO NOTHING
        """
        
        args_list = [
            (
                msg['user_id'],
                msg['message_id'],
                msg['text'],
                msg['message_type'],
                msg['created_at']
            )
            for msg in messages
        ]
        
        await self.pool.execute_many(query, args_list)
    
    async def get_active_users_batch(self, limit: int = 1000, offset: int = 0) -> List[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª Ú©Ø§Ø±Ø¨Ø±Ø§Ù† ÙØ¹Ø§Ù„ Ø¨Ù‡ ØµÙˆØ±Øª batch"""
        query = """
        SELECT user_id, first_name, last_name, username
        FROM users
        WHERE is_active = true
        ORDER BY last_activity_at DESC
        LIMIT $1 OFFSET $2
        """
        
        return await self.pool.execute_query(query, limit, offset)
    
    async def update_user_activity_batch(self, user_activities: List[tuple]):
        """Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ batch ÙØ¹Ø§Ù„ÛŒØª Ú©Ø§Ø±Ø¨Ø±Ø§Ù†"""
        query = """
        UPDATE users 
        SET last_activity_at = $2, message_count = message_count + 1
        WHERE user_id = $1
        """
        
        await self.pool.execute_many(query, user_activities)
    
    async def cleanup_old_data(self, days: int = 30):
        """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        queries = [
            (
                "DELETE FROM messages WHERE created_at < NOW() - INTERVAL '%s days'",
                (days,)
            ),
            (
                "DELETE FROM user_sessions WHERE created_at < NOW() - INTERVAL '%s days'",
                (days,)
            )
        ]
        
        await self.pool.execute_transaction(queries)
```

### 2. Database Indexing

#### Index Management
```sql
-- optimizations/indexes.sql

-- Index Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_active 
ON users (is_active, last_activity_at DESC) 
WHERE is_active = true;

-- Index Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_messages_user_created 
ON messages (user_id, created_at DESC);

-- Index Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…ØªÙ†ÛŒ
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_messages_text_search 
ON messages USING gin(to_tsvector('english', text));

-- Index Ø¨Ø±Ø§ÛŒ Ø¢Ù…Ø§Ø±
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_messages_type_created 
ON messages (message_type, created_at);

-- Partial index Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_messages_recent 
ON messages (created_at DESC) 
WHERE created_at > NOW() - INTERVAL '7 days';

-- Index Ø¨Ø±Ø§ÛŒ user settings
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_settings_lookup 
ON user_settings (user_id, setting_key);

-- Composite index Ø¨Ø±Ø§ÛŒ query Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_stats 
ON users (is_active, created_at, last_activity_at) 
WHERE is_active = true;
```

#### Index Monitoring
```python
# optimizations/index_monitor.py
import asyncio
from typing import List, Dict

class IndexMonitor:
    """Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ index Ù‡Ø§ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"""
    
    def __init__(self, db_pool: DatabasePool):
        self.db_pool = db_pool
    
    async def get_index_usage_stats(self) -> List[Dict]:
        """Ø¢Ù…Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² index Ù‡Ø§"""
        query = """
        SELECT 
            schemaname,
            tablename,
            indexname,
            idx_tup_read,
            idx_tup_fetch,
            idx_scan,
            idx_tup_read::float / NULLIF(idx_scan, 0) as avg_tuples_per_scan
        FROM pg_stat_user_indexes
        ORDER BY idx_scan DESC;
        """
        
        return await self.db_pool.execute_query(query)
    
    async def get_unused_indexes(self) -> List[Dict]:
        """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† index Ù‡Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡"""
        query = """
        SELECT 
            schemaname,
            tablename,
            indexname,
            pg_size_pretty(pg_relation_size(indexrelid)) as size
        FROM pg_stat_user_indexes
        WHERE idx_scan = 0
        AND schemaname = 'public';
        """
        
        return await self.db_pool.execute_query(query)
    
    async def get_table_sizes(self) -> List[Dict]:
        """Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¬Ø¯Ø§ÙˆÙ„"""
        query = """
        SELECT 
            tablename,
            pg_size_pretty(pg_total_relation_size(tablename::regclass)) as total_size,
            pg_size_pretty(pg_relation_size(tablename::regclass)) as table_size,
            pg_size_pretty(pg_total_relation_size(tablename::regclass) - pg_relation_size(tablename::regclass)) as index_size
        FROM pg_tables
        WHERE schemaname = 'public'
        ORDER BY pg_total_relation_size(tablename::regclass) DESC;
        """
        
        return await self.db_pool.execute_query(query)
```

## ğŸ§  Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡

### 1. Memory Management

#### Object Pooling
```python
# optimizations/object_pool.py
import asyncio
from typing import TypeVar, Generic, Callable, Optional, Set
from collections import deque
import weakref

T = TypeVar('T')

class ObjectPool(Generic[T]):
    """Pool Ø§Ø´ÛŒØ§Ø¡ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ memory allocation"""
    
    def __init__(self, factory: Callable[[], T], max_size: int = 100):
        self.factory = factory
        self.max_size = max_size
        self._pool: deque = deque()
        self._in_use: Set[T] = set()
        self._lock = asyncio.Lock()
    
    async def acquire(self) -> T:
        """Ø¯Ø±ÛŒØ§ÙØª Ø´ÛŒØ¡ Ø§Ø² pool"""
        async with self._lock:
            if self._pool:
                obj = self._pool.popleft()
            else:
                obj = self.factory()
            
            self._in_use.add(obj)
            return obj
    
    async def release(self, obj: T):
        """Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø´ÛŒØ¡ Ø¨Ù‡ pool"""
        async with self._lock:
            if obj in self._in_use:
                self._in_use.remove(obj)
                
                if len(self._pool) < self.max_size:
                    # Reset object state if needed
                    if hasattr(obj, 'reset'):
                        obj.reset()
                    self._pool.append(obj)
    
    def stats(self) -> dict:
        """Ø¢Ù…Ø§Ø± pool"""
        return {
            'pool_size': len(self._pool),
            'in_use': len(self._in_use),
            'max_size': self.max_size
        }

# Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡
class MessageProcessor:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ù¾ÛŒØ§Ù… Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø¬Ø¯Ø¯"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """Ø±ÛŒØ³Øª Ú©Ø±Ø¯Ù† ÙˆØ¶Ø¹ÛŒØª"""
        self.processed_count = 0
        self.errors = []
    
    async def process(self, message):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ§Ù…"""
        self.processed_count += 1
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ§Ù…
        return f"Processed: {message}"

# Ø§ÛŒØ¬Ø§Ø¯ pool
message_processor_pool = ObjectPool(MessageProcessor, max_size=50)
```

#### Memory Monitoring
```python
# optimizations/memory_monitor.py
import psutil
import gc
import sys
from typing import Dict, List
import tracemalloc
import logging

class MemoryMonitor:
    """Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡"""
    
    def __init__(self):
        self.process = psutil.Process()
        self.baseline_memory = None
    
    def get_memory_info(self) -> Dict[str, float]:
        """Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø­Ø§ÙØ¸Ù‡"""
        memory_info = self.process.memory_info()
        
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,
            'vms_mb': memory_info.vms / 1024 / 1024,
            'percent': self.process.memory_percent(),
            'available_mb': psutil.virtual_memory().available / 1024 / 1024
        }
    
    def set_baseline(self):
        """ØªÙ†Ø¸ÛŒÙ… baseline Ø­Ø§ÙØ¸Ù‡"""
        self.baseline_memory = self.get_memory_info()
    
    def get_memory_diff(self) -> Dict[str, float]:
        """ØªÙØ§ÙˆØª Ø¨Ø§ baseline"""
        if not self.baseline_memory:
            return {}
        
        current = self.get_memory_info()
        return {
            key: current[key] - self.baseline_memory[key]
            for key in current.keys()
        }
    
    def force_garbage_collection(self) -> Dict[str, int]:
        """Ø§Ø¬Ø¨Ø§Ø± garbage collection"""
        before_objects = len(gc.get_objects())
        
        # Ø§Ø¬Ø±Ø§ÛŒ garbage collection
        collected = gc.collect()
        
        after_objects = len(gc.get_objects())
        
        return {
            'collected': collected,
            'objects_before': before_objects,
            'objects_after': after_objects,
            'objects_freed': before_objects - after_objects
        }
    
    def get_top_memory_objects(self, limit: int = 10) -> List[Dict]:
        """Ø§Ø´ÛŒØ§Ø¡ Ù¾Ø±Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡"""
        if not tracemalloc.is_tracing():
            tracemalloc.start()
        
        snapshot = tracemalloc.take_snapshot()
        top_stats = snapshot.statistics('lineno')
        
        result = []
        for stat in top_stats[:limit]:
            result.append({
                'filename': stat.traceback.format()[-1],
                'size_mb': stat.size / 1024 / 1024,
                'count': stat.count
            })
        
        return result
    
    async def monitor_continuously(self, interval: int = 60):
        """Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ù…Ø¯Ø§ÙˆÙ…"""
        while True:
            memory_info = self.get_memory_info()
            
            # Ù‡Ø´Ø¯Ø§Ø± Ø¯Ø± ØµÙˆØ±Øª Ù…ØµØ±Ù Ø¨Ø§Ù„Ø§ÛŒ Ø­Ø§ÙØ¸Ù‡
            if memory_info['percent'] > 80:
                logging.warning(f"High memory usage: {memory_info['percent']:.1f}%")
                
                # Ø§Ø¬Ø±Ø§ÛŒ garbage collection
                gc_stats = self.force_garbage_collection()
                logging.info(f"Garbage collection: {gc_stats}")
            
            await asyncio.sleep(interval)

# Global memory monitor
memory_monitor = MemoryMonitor()
```

### 2. Data Structure Optimization

#### Efficient Data Structures
```python
# optimizations/data_structures.py
import array
from collections import defaultdict, deque
from typing import Any, Dict, List, Optional
import bisect

class CircularBuffer:
    """Buffer Ø¯Ø§ÛŒØ±Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­Ø¯ÙˆØ¯ Ø¯Ø§Ø¯Ù‡"""
    
    def __init__(self, max_size: int):
        self.max_size = max_size
        self.buffer = deque(maxlen=max_size)
    
    def append(self, item: Any):
        """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¢ÛŒØªÙ…"""
        self.buffer.append(item)
    
    def get_all(self) -> List[Any]:
        """Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§"""
        return list(self.buffer)
    
    def get_recent(self, count: int) -> List[Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±"""
        return list(self.buffer)[-count:]
    
    def clear(self):
        """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† buffer"""
        self.buffer.clear()

class LRUCache:
    """LRU Cache Ø¨Ù‡ÛŒÙ†Ù‡"""
    
    def __init__(self, max_size: int):
        self.max_size = max_size
        self.cache: Dict[Any, Any] = {}
        self.access_order = deque()
    
    def get(self, key: Any) -> Optional[Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø² cache"""
        if key in self.cache:
            # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ØªØ±ØªÛŒØ¨ Ø¯Ø³ØªØ±Ø³ÛŒ
            self.access_order.remove(key)
            self.access_order.append(key)
            return self.cache[key]
        return None
    
    def put(self, key: Any, value: Any):
        """Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù† Ø¯Ø± cache"""
        if key in self.cache:
            # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ù‚Ø¯Ø§Ø± Ù…ÙˆØ¬ÙˆØ¯
            self.cache[key] = value
            self.access_order.remove(key)
            self.access_order.append(key)
        else:
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø¯Ø§Ø± Ø¬Ø¯ÛŒØ¯
            if len(self.cache) >= self.max_size:
                # Ø­Ø°Ù Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† Ø¢ÛŒØªÙ…
                oldest_key = self.access_order.popleft()
                del self.cache[oldest_key]
            
            self.cache[key] = value
            self.access_order.append(key)

class CompactUserData:
    """Ø³Ø§Ø®ØªØ§Ø± ÙØ´Ø±Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±"""
    
    def __init__(self):
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² array Ø¨Ø±Ø§ÛŒ Ø§Ø¹Ø¯Ø§Ø¯
        self.user_ids = array.array('q')  # long long
        self.message_counts = array.array('i')  # int
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² dict Ø¨Ø±Ø§ÛŒ string Ù‡Ø§
        self.usernames: Dict[int, str] = {}
        self.first_names: Dict[int, str] = {}
    
    def add_user(self, user_id: int, username: str, first_name: str, message_count: int):
        """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ú©Ø§Ø±Ø¨Ø±"""
        index = len(self.user_ids)
        
        self.user_ids.append(user_id)
        self.message_counts.append(message_count)
        
        if username:
            self.usernames[index] = username
        if first_name:
            self.first_names[index] = first_name
    
    def get_user(self, index: int) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ú©Ø§Ø±Ø¨Ø±"""
        if index >= len(self.user_ids):
            return {}
        
        return {
            'user_id': self.user_ids[index],
            'message_count': self.message_counts[index],
            'username': self.usernames.get(index, ''),
            'first_name': self.first_names.get(index, '')
        }
    
    def memory_usage(self) -> Dict[str, int]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡"""
        return {
            'user_ids_bytes': self.user_ids.buffer_info()[1] * self.user_ids.itemsize,
            'message_counts_bytes': self.message_counts.buffer_info()[1] * self.message_counts.itemsize,
            'usernames_estimate': sum(len(s.encode('utf-8')) for s in self.usernames.values()),
            'first_names_estimate': sum(len(s.encode('utf-8')) for s in self.first_names.values())
        }
```

## ğŸŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¨Ú©Ù‡

### 1. HTTP Optimization

#### Request Batching
```python
# optimizations/http_batch.py
import asyncio
import aiohttp
from typing import List, Dict, Any, Optional
import time

class HTTPBatcher:
    """Batch Ú©Ø±Ø¯Ù† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ HTTP"""
    
    def __init__(self, batch_size: int = 10, flush_interval: float = 1.0):
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.pending_requests: List[Dict] = []
        self.last_flush = time.time()
        self._lock = asyncio.Lock()
        self._session: Optional[aiohttp.ClientSession] = None
    
    async def __aenter__(self):
        connector = aiohttp.TCPConnector(
            limit=100,
            limit_per_host=30,
            ttl_dns_cache=300,
            use_dns_cache=True
        )
        
        self._session = aiohttp.ClientSession(
            connector=connector,
            timeout=aiohttp.ClientTimeout(total=30)
        )
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.flush()
        if self._session:
            await self._session.close()
    
    async def add_request(self, method: str, url: str, **kwargs) -> asyncio.Future:
        """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ batch"""
        future = asyncio.Future()
        
        async with self._lock:
            self.pending_requests.append({
                'method': method,
                'url': url,
                'kwargs': kwargs,
                'future': future
            })
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ flush
            should_flush = (
                len(self.pending_requests) >= self.batch_size or
                time.time() - self.last_flush >= self.flush_interval
            )
            
            if should_flush:
                await self._flush_batch()
        
        return future
    
    async def _flush_batch(self):
        """Ø§Ø¬Ø±Ø§ÛŒ batch Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§"""
        if not self.pending_requests:
            return
        
        batch = self.pending_requests.copy()
        self.pending_requests.clear()
        self.last_flush = time.time()
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ù…ÙˆØ§Ø²ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
        tasks = []
        for request in batch:
            task = asyncio.create_task(
                self._execute_request(request)
            )
            tasks.append(task)
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _execute_request(self, request: Dict):
        """Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ© Ø¯Ø±Ø®ÙˆØ§Ø³Øª"""
        try:
            async with self._session.request(
                request['method'],
                request['url'],
                **request['kwargs']
            ) as response:
                result = await response.json()
                request['future'].set_result(result)
        except Exception as e:
            request['future'].set_exception(e)
    
    async def flush(self):
        """Ø§Ø¬Ø¨Ø§Ø± flush Ú©Ø±Ø¯Ù† batch"""
        async with self._lock:
            await self._flush_batch()
```

### 2. Connection Optimization

#### Connection Pooling
```python
# optimizations/connection_pool.py
import asyncio
import aiohttp
from typing import Optional, Dict, Any
import ssl
import certifi

class OptimizedHTTPClient:
    """HTTP client Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡"""
    
    def __init__(self, 
                 max_connections: int = 100,
                 max_connections_per_host: int = 30,
                 keepalive_timeout: int = 30,
                 timeout: int = 30):
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª SSL
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_REQUIRED
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª connector
        self.connector = aiohttp.TCPConnector(
            limit=max_connections,
            limit_per_host=max_connections_per_host,
            ttl_dns_cache=300,
            use_dns_cache=True,
            keepalive_timeout=keepalive_timeout,
            enable_cleanup_closed=True,
            ssl=ssl_context
        )
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª timeout
        self.timeout = aiohttp.ClientTimeout(
            total=timeout,
            connect=10,
            sock_read=timeout
        )
        
        self.session: Optional[aiohttp.ClientSession] = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=self.timeout,
            headers={
                'User-Agent': 'TelegramBot/1.0',
                'Accept': 'application/json',
                'Accept-Encoding': 'gzip, deflate'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def get(self, url: str, **kwargs) -> Dict[str, Any]:
        """GET request Ø¨Ù‡ÛŒÙ†Ù‡"""
        async with self.session.get(url, **kwargs) as response:
            response.raise_for_status()
            return await response.json()
    
    async def post(self, url: str, **kwargs) -> Dict[str, Any]:
        """POST request Ø¨Ù‡ÛŒÙ†Ù‡"""
        async with self.session.post(url, **kwargs) as response:
            response.raise_for_status()
            return await response.json()
    
    def get_connection_stats(self) -> Dict[str, Any]:
        """Ø¢Ù…Ø§Ø± Ø§ØªØµØ§Ù„Ø§Øª"""
        if self.connector:
            return {
                'total_connections': len(self.connector._conns),
                'available_connections': len(self.connector._available_connections),
                'acquired_connections': len(self.connector._acquired),
                'closed_connections': len(self.connector._closed)
            }
        return {}
```

## ğŸ“Š Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø¹Ù…Ù„Ú©Ø±Ø¯

### 1. Real-time Monitoring

#### Performance Dashboard
```python
# monitoring/dashboard.py
import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any
import aiohttp
from aiohttp import web

class PerformanceDashboard:
    """Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ real-time"""
    
    def __init__(self, port: int = 8080):
        self.port = port
        self.app = web.Application()
        self.setup_routes()
        self.metrics_history: List[Dict] = []
    
    def setup_routes(self):
        """ØªÙ†Ø¸ÛŒÙ… route Ù‡Ø§"""
        self.app.router.add_get('/', self.dashboard_handler)
        self.app.router.add_get('/api/metrics', self.metrics_handler)
        self.app.router.add_get('/api/health', self.health_handler)
        self.app.router.add_static('/static', 'static')
    
    async def dashboard_handler(self, request):
        """ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯"""
        html = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>Bot Performance Dashboard</title>
            <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .metric-card { 
                    border: 1px solid #ddd; 
                    padding: 15px; 
                    margin: 10px; 
                    border-radius: 5px; 
                    display: inline-block;
                    min-width: 200px;
                }
                .chart-container { width: 80%; margin: 20px auto; }
            </style>
        </head>
        <body>
            <h1>ğŸ¤– Bot Performance Dashboard</h1>
            
            <div id="metrics-cards"></div>
            
            <div class="chart-container">
                <canvas id="responseTimeChart"></canvas>
            </div>
            
            <div class="chart-container">
                <canvas id="memoryChart"></canvas>
            </div>
            
            <script>
                // Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ metrics Ù‡Ø± 5 Ø«Ø§Ù†ÛŒÙ‡
                setInterval(updateMetrics, 5000);
                updateMetrics();
                
                async function updateMetrics() {
                    try {
                        const response = await fetch('/api/metrics');
                        const data = await response.json();
                        updateMetricsCards(data);
                        updateCharts(data);
                    } catch (error) {
                        console.error('Error fetching metrics:', error);
                    }
                }
                
                function updateMetricsCards(data) {
                    const container = document.getElementById('metrics-cards');
                    container.innerHTML = `
                        <div class="metric-card">
                            <h3>Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡</h3>
                            <p>${data.messages_processed || 0}</p>
                        </div>
                        <div class="metric-card">
                            <h3>Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø® Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†</h3>
                            <p>${(data.avg_response_time || 0).toFixed(2)} ms</p>
                        </div>
                        <div class="metric-card">
                            <h3>Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡</h3>
                            <p>${(data.memory_usage || 0).toFixed(1)} MB</p>
                        </div>
                        <div class="metric-card">
                            <h3>Ú©Ø§Ø±Ø¨Ø±Ø§Ù† ÙØ¹Ø§Ù„</h3>
                            <p>${data.active_users || 0}</p>
                        </div>
                    `;
                }
                
                function updateCharts(data) {
                    // Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§
                }
            </script>
        </body>
        </html>
        """
        return web.Response(text=html, content_type='text/html')
    
    async def metrics_handler(self, request):
        """API metrics"""
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ metrics Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø®ØªÙ„Ù
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'messages_processed': perf_monitor.counters.get('message_success', 0),
            'avg_response_time': self._get_avg_response_time(),
            'memory_usage': memory_monitor.get_memory_info()['rss_mb'],
            'active_users': await self._get_active_users_count(),
            'cache_hit_ratio': memory_cache.stats()['hit_ratio'],
            'database_connections': db_pool.pool._holders.__len__() if db_pool.pool else 0
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
        self.metrics_history.append(metrics)
        
        # Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ† ÙÙ‚Ø· 100 Ø±Ú©ÙˆØ±Ø¯ Ø§Ø®ÛŒØ±
        if len(self.metrics_history) > 100:
            self.metrics_history = self.metrics_history[-100:]
        
        return web.json_response(metrics)
    
    async def health_handler(self, request):
        """Health check endpoint"""
        health_status = {
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'uptime': time.time() - start_time,
            'version': '1.0.0'
        }
        
        return web.json_response(health_status)
    
    def _get_avg_response_time(self) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®"""
        stats = perf_monitor.get_stats('message_processing')
        return stats.get('avg', 0) * 1000  # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡
    
    async def _get_active_users_count(self) -> int:
        """ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† ÙØ¹Ø§Ù„"""
        # Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø·Ù‚ Ø¨Ø±Ù†Ø§Ù…Ù‡
        return 0
    
    async def start_server(self):
        """Ø´Ø±ÙˆØ¹ Ø³Ø±ÙˆØ± Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯"""
        runner = web.AppRunner(self.app)
        await runner.setup()
        
        site = web.TCPSite(runner, 'localhost', self.port)
        await site.start()
        
        print(f"ğŸ“Š Performance dashboard started on http://localhost:{self.port}")

# Global dashboard instance
dashboard = PerformanceDashboard()
```

## ğŸ”— Ù…Ø±Ø§Ø­Ù„ Ø¨Ø¹Ø¯ÛŒ

- [Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ùˆ Ù†Ø¸Ø§Ø±Øª](MONITORING.md)
- [Ø§Ù…Ù†ÛŒØª Ù¾ÛŒØ´Ø±ÙØªÙ‡](SECURITY.md)
- [Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ](SCALING.md)